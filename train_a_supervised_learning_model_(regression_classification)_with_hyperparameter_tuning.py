# -*- coding: utf-8 -*-
"""Train a Supervised Learning Model (Regression / Classification) with Hyperparameter Tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1641OQ_U3IYR7zYu3D1JnbqBZ1PG9dcBk
"""

# ==============================
# Supervised Learning with Hyperparameter Tuning
# Works with /content/drive/MyDrive/saas_churn_data.csv
# ==============================

# ðŸ“¦ Install & Import Libraries
!pip install scikit-learn joblib seaborn --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,
    mean_squared_error, r2_score
)

# -------------------------------
# 1. Load Dataset
# -------------------------------
file_path = "/content/drive/MyDrive/saas_churn_data.csv"
df = pd.read_csv(file_path)

print("âœ… Dataset loaded successfully!")
print(df.head())
print("\nShape:", df.shape)

# -------------------------------
# Choose Task Type: "classification" or "regression"
# -------------------------------
task_type = "classification"   # change to "regression" if target is numeric

# -------------------------------
# 2. Preprocess Data
# -------------------------------
# Assume target column is named 'churn' (change if different in your dataset)
target_col = "churn"

# Convert categorical churn to numeric if needed
if task_type == "classification":
    if df[target_col].dtype == 'object':
        df[target_col] = df[target_col].map({"Yes": 1, "No": 0}).fillna(df[target_col])

# Handle categorical features (one-hot encode if needed)
df = pd.get_dummies(df, drop_first=True)

# Split features & target
X = df.drop(columns=[target_col])
y = df[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("\nâœ… Data split complete:")
print("Train size:", X_train.shape, "Test size:", X_test.shape)

# -------------------------------
# 3. Model Training + Hyperparameter Tuning
# -------------------------------
if task_type == "classification":
    # Baseline model
    base_model = RandomForestClassifier(random_state=42)
    base_model.fit(X_train, y_train)

    # Hyperparameter tuning
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 5, 10]
    }
    grid = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1
    )
    grid.fit(X_train, y_train)
    tuned_model = grid.best_estimator_
    print("\nðŸŽ¯ Best Parameters:", grid.best_params_)

else:  # regression
    # Baseline model
    base_model = DecisionTreeRegressor(random_state=42)
    base_model.fit(X_train, y_train)

    # Hyperparameter tuning
    param_grid = {
        'max_depth': [None, 5, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 5]
    }
    grid = GridSearchCV(
        DecisionTreeRegressor(random_state=42),
        param_grid,
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )
    grid.fit(X_train, y_train)
    tuned_model = grid.best_estimator_
    print("\nðŸŽ¯ Best Parameters:", grid.best_params_)

# -------------------------------
# 4. Evaluation
# -------------------------------
def evaluate_classification(model, X_test, y_test, title="Model Evaluation"):
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average="weighted")
    rec = recall_score(y_test, y_pred, average="weighted")
    f1 = f1_score(y_test, y_pred, average="weighted")

    print(f"\nðŸ”¹ {title}")
    print(classification_report(y_test, y_pred))
    print(f"âœ… Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}")

    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(f"{title} - Confusion Matrix")
    plt.show()

def evaluate_regression(model, X_test, y_test, title="Model Evaluation"):
    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"\nðŸ”¹ {title}")
    print(f"âœ… MSE: {mse:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}")

    plt.figure(figsize=(6,5))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
    plt.xlabel("Actual Values")
    plt.ylabel("Predicted Values")
    plt.title(f"{title} - Predictions vs Actuals")
    plt.show()

# Evaluate baseline & tuned
if task_type == "classification":
    evaluate_classification(base_model, X_test, y_test, "Baseline Model")
    evaluate_classification(tuned_model, X_test, y_test, "Tuned Model")
else:
    evaluate_regression(base_model, X_test, y_test, "Baseline Model")
    evaluate_regression(tuned_model, X_test, y_test, "Tuned Model")

# -------------------------------
# 5. Save Model
# -------------------------------
model_filename = f"{task_type}_model.pkl"
joblib.dump(tuned_model, model_filename)
print(f"\nâœ… Model saved as {model_filename}")